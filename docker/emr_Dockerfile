FROM public.ecr.aws/emr-on-eks/spark/emr-7.2.0

USER root
# 기본 환경 변수 설정
ENV AWS_REGION=ap-northeast-2 \
    SPARK_HOME=/usr/lib/spark \
    HADOOP_HOME=/usr/lib/hadoop \
    JAVA_HOME=/etc/alternatives/jre \
    PYSPARK_PYTHON=/usr/bin/python3

# 필수 패키지 설치
RUN yum update -y && \
    yum install -y \
    sudo \
    curl \
    jq \
    unzip \
    git \
    tar --allowerasing && \
    yum clean all

# PySpark 및 추가 Python 패키지 설치
RUN pip3 install --no-cache-dir \
    pyspark==3.5.1 \
    boto3 \
    requests \
    pandas \
    numpy \
    fastparquet \
    pyarrow \ 
    PyPDF2


# JAR 파일을 /opt/spark/jars/에 복사
# Spark, aws-java-sdk, hadoop-aws 간의 버전 호환성이 중요하다.
RUN mkdir -p /opt/spark/jars && \
    curl -o /opt/spark/jars/aws-java-sdk-bundle-1.11.901.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.901/aws-java-sdk-bundle-1.11.901.jar && \
    curl -o /opt/spark/jars/hadoop-aws-3.3.1.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.1/hadoop-aws-3.3.1.jar


# Hadoop AWS 관련 설정 추가
RUN mkdir -p $HADOOP_HOME/etc/hadoop && \
    echo "export HADOOP_HOME=$HADOOP_HOME" >> ~/.bashrc && \
    echo "export SPARK_HOME=$SPARK_HOME" >> ~/.bashrc && \
    echo "export PATH=$SPARK_HOME/bin:$HADOOP_HOME/bin:$PATH" >> ~/.bashrc

# S3 접근을 위한 AWS CLI 설치
RUN curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" && \
    unzip awscliv2.zip && \
    ./aws/install && \
    rm -rf awscliv2.zip aws

ENV PYTHONPATH="/usr/local/lib/python3.8/site-packages:/usr/lib/python3.9/site-packages"

# Hadoop 및 Spark 관련 환경 변수 설정
ENV HADOOP_CONF_DIR=/etc/hadoop/conf \
    SPARK_CONF_DIR=/etc/spark/conf \
    PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

# 작업 디렉토리 설정
# 로컬 개발환경이면 USER hadoop:hadoop 지우고 컨테이너에서 su - hadoop 으로 사용
USER hadoop:hadoop
WORKDIR /home/hadoop

